{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.2.4\n",
      "/home/ec2-user/SageMaker\r\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import warnings\n",
    "import tarfile\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, LeakyReLU\n",
    "from keras.layers import BatchNormalization, Activation, Conv2D\n",
    "from keras.layers import GlobalAveragePooling2D, Lambda\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from cv2 import resize\n",
    "\n",
    "import tensorflow as tfw\n",
    "from keras.applications import ResNet50\n",
    "from keras import regularizers\n",
    "import requests\n",
    "import threading\n",
    "import random\n",
    "import time\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('Keras version:', keras.__version__)\n",
    "# print(os.listdir('SageMaker'))\n",
    "\n",
    "warnings.simplefilter('default')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/landmarks/train/train/'\n",
    "test_path = './data/landmarks/test/test/'\n",
    "train_images = glob.glob(train_path+'*.jpg')\n",
    "test_images = glob.glob(test_path+'*.jpg')\n",
    "print(len(train_images))\n",
    "print(len(test_images))\n",
    "sample_submission = pd.read_csv('./data/landmarks/recognition_sample_submission.csv')\n",
    "sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_ids = [image_file.replace(\n",
    "    '.jpg', '').replace(train_path, '') for image_file in train_images]\n",
    "\n",
    "train_df = pd.DataFrame(index=list(range(0,len(train_image_ids))))\n",
    "train_df['filename'] = pd.Series(train_images, index=list(range(0,len(train_image_ids))))\n",
    "train_df['ids'] = train_image_ids\n",
    "test_image_ids = [image_file.replace(\n",
    "    '.jpg', '').replace(test_path, '') for image_file in test_images]\n",
    "test_df = pd.DataFrame(index=list(range(0,len(test_image_ids))))\n",
    "test_df['filename'] = pd.Series(test_images, index=list(range(0,len(test_image_ids))))\n",
    "test_df['ids'] = test_image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\",index_col='id')\n",
    "print(train.head())\n",
    "print(train.shape)\n",
    "print(\"Number of classes {}\".format(len(train.landmark_id.unique())))\n",
    "\n",
    "NUM_THRESHOLD = 420\n",
    "\n",
    "counts = dict(Counter(train['landmark_id']))\n",
    "landmarks_dict = {x:[] for x in train.landmark_id.unique() if counts[x] >= NUM_THRESHOLD}\n",
    "NUM_CLASSES = len(landmarks_dict)\n",
    "print(\"Total number of valid classes: {}\".format(NUM_CLASSES))\n",
    "\n",
    "i = 0\n",
    "landmark_to_idx = {}\n",
    "idx_to_landmark = []\n",
    "for k in landmarks_dict:\n",
    "    landmark_to_idx[k] = i\n",
    "    idx_to_landmark.append(k)\n",
    "    i += 1\n",
    "    \n",
    "train['filename'] = pd.Series(train_images, index=train_image_ids)\n",
    "\n",
    "all_urls = train['url'].tolist()\n",
    "all_landmarks = train['landmark_id'].tolist()\n",
    "valid_urls_dict = {x[0].split(\"/\")[-1]:landmark_to_idx[x[1]] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict}\n",
    "valid_urls_list = [x[0] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict]\n",
    "\n",
    "NUM_EXAMPLES = len(valid_urls_list)\n",
    "print(\"Total number of valid examples: {}\".format(NUM_EXAMPLES)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info_full = pd.read_csv('./data/test.csv', index_col='id')\n",
    "test_info_full.head()\n",
    "\n",
    "test_info = test_info_full.loc[test_image_ids]\n",
    "test_info['filename'] = pd.Series(test_images, index=test_image_ids)\n",
    "\n",
    "test_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
