{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.2.4\n",
      "/home/ec2-user/SageMaker\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  return f(*args, **kwds)\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0517 18:40:03.740313 139693473142592 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import warnings\n",
    "import tarfile\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, LeakyReLU\n",
    "from keras.layers import BatchNormalization, Activation, Conv2D\n",
    "from keras.layers import GlobalAveragePooling2D, Lambda\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from cv2 import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.applications import ResNet50\n",
    "from keras import regularizers\n",
    "import requests\n",
    "import threading\n",
    "import random\n",
    "import time\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('Keras version:', keras.__version__)\n",
    "# print(os.listdir('SageMaker'))\n",
    "\n",
    "warnings.simplefilter('default')\n",
    "!pwd\n",
    "import argparse\n",
    "import glob\n",
    "import pickle\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from skimage.feature import plot_matches\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import AffineTransform\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.python.platform import app\n",
    "\n",
    "warnings.simplefilter('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4130318\n",
      "112821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(117703, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = './data/landmarks/train/train/'\n",
    "test_path = './data/landmarks/test/test/'\n",
    "train_images = glob.glob(train_path+'*.jpg')\n",
    "test_images = glob.glob(test_path+'*.jpg')\n",
    "print(len(train_images))\n",
    "print(len(test_images))\n",
    "sample_submission = pd.read_csv('./data/landmarks/recognition_sample_submission.csv')\n",
    "sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_ids = [image_file.replace(\n",
    "    '.jpg', '').replace(train_path, '') for image_file in train_images]\n",
    "\n",
    "train_df = pd.DataFrame(index=list(range(0,len(train_image_ids))))\n",
    "train_df['filename'] = pd.Series(train_images, index=list(range(0,len(train_image_ids))))\n",
    "train_df['ids'] = train_image_ids\n",
    "test_image_ids = [image_file.replace(\n",
    "    '.jpg', '').replace(test_path, '') for image_file in test_images]\n",
    "test_df = pd.DataFrame(index=list(range(0,len(test_image_ids))))\n",
    "test_df['filename'] = pd.Series(test_images, index=list(range(0,len(test_image_ids))))\n",
    "test_df['ids'] = test_image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                url  \\\n",
      "id                                                                    \n",
      "6e158a47eb2ca3f6  https://upload.wikimedia.org/wikipedia/commons...   \n",
      "202cd79556f30760  http://upload.wikimedia.org/wikipedia/commons/...   \n",
      "3ad87684c99c06e1  http://upload.wikimedia.org/wikipedia/commons/...   \n",
      "e7f70e9c61e66af3  https://upload.wikimedia.org/wikipedia/commons...   \n",
      "4072182eddd0100e  https://upload.wikimedia.org/wikipedia/commons...   \n",
      "\n",
      "                  landmark_id  \n",
      "id                             \n",
      "6e158a47eb2ca3f6       142820  \n",
      "202cd79556f30760       104169  \n",
      "3ad87684c99c06e1        37914  \n",
      "e7f70e9c61e66af3       102140  \n",
      "4072182eddd0100e         2474  \n",
      "(4132914, 2)\n",
      "Number of classes 203094\n",
      "Total number of valid classes: 23215\n",
      "url            0\n",
      "landmark_id    0\n",
      "filename       0\n",
      "dtype: int64\n",
      "Total number of valid examples: 2296997\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\",index_col='id')\n",
    "print(train.head())\n",
    "print(train.shape)\n",
    "print(\"Number of classes {}\".format(len(train.landmark_id.unique())))\n",
    "\n",
    "NUM_THRESHOLD = 42\n",
    "\n",
    "counts = dict(Counter(train['landmark_id']))\n",
    "landmarks_dict = {x:[] for x in train.landmark_id.unique() if counts[x] >= NUM_THRESHOLD}\n",
    "NUM_CLASSES = len(landmarks_dict)\n",
    "print(\"Total number of valid classes: {}\".format(NUM_CLASSES))\n",
    "\n",
    "i = 0\n",
    "landmark_to_idx = {}\n",
    "idx_to_landmark = []\n",
    "for k in landmarks_dict:\n",
    "    landmark_to_idx[k] = i\n",
    "    idx_to_landmark.append(k)\n",
    "    i += 1\n",
    "    \n",
    "train['filename'] = pd.Series(train_images, index=train_image_ids)\n",
    "\n",
    "train = train.dropna(axis=0)\n",
    "print(train.isna().sum())\n",
    "\n",
    "all_urls = train['url'].tolist()\n",
    "all_filenames= train['filename'].tolist()\n",
    "all_landmarks = train['landmark_id'].tolist()\n",
    "valid_urls_dict = {x[0].split(\"/\")[-1]:landmark_to_idx[x[1]] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict}\n",
    "valid_filenames_dict = {x[0].split('/')[-1]:landmark_to_idx[x[1]] for x in zip(all_filenames, all_landmarks) if x[1] in landmarks_dict}\n",
    "valid_urls_list = [x[0] for x in zip(all_urls, all_landmarks) if x[1] in landmarks_dict]\n",
    "valid_filenames_list = [x[0] for x in zip(all_filenames, all_landmarks) if x[1] in landmarks_dict]\n",
    "\n",
    "NUM_EXAMPLES = len(valid_urls_list)\n",
    "print(\"Total number of valid examples: {}\".format(NUM_EXAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fa8d5a81a16f2f2f</th>\n",
       "      <td>https://lh3.googleusercontent.com/-_SAJTBt3Y64...</td>\n",
       "      <td>./data/landmarks/test/test/fa8d5a81a16f2f2f.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b81a0a45f9b1ee97</th>\n",
       "      <td>https://lh3.googleusercontent.com/-9sFSIOCzIOs...</td>\n",
       "      <td>./data/landmarks/test/test/b81a0a45f9b1ee97.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570e28cc63fab858</th>\n",
       "      <td>https://lh3.googleusercontent.com/-7Eld7yUfAB0...</td>\n",
       "      <td>./data/landmarks/test/test/570e28cc63fab858.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b8bc63608b5fef1a</th>\n",
       "      <td>https://lh3.googleusercontent.com/-JdgzGjeS9NE...</td>\n",
       "      <td>./data/landmarks/test/test/b8bc63608b5fef1a.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54cfd1f5f683b966</th>\n",
       "      <td>https://lh3.googleusercontent.com/-krCM7YZ3FpU...</td>\n",
       "      <td>./data/landmarks/test/test/54cfd1f5f683b966.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                url  \\\n",
       "id                                                                    \n",
       "fa8d5a81a16f2f2f  https://lh3.googleusercontent.com/-_SAJTBt3Y64...   \n",
       "b81a0a45f9b1ee97  https://lh3.googleusercontent.com/-9sFSIOCzIOs...   \n",
       "570e28cc63fab858  https://lh3.googleusercontent.com/-7Eld7yUfAB0...   \n",
       "b8bc63608b5fef1a  https://lh3.googleusercontent.com/-JdgzGjeS9NE...   \n",
       "54cfd1f5f683b966  https://lh3.googleusercontent.com/-krCM7YZ3FpU...   \n",
       "\n",
       "                                                         filename  \n",
       "id                                                                 \n",
       "fa8d5a81a16f2f2f  ./data/landmarks/test/test/fa8d5a81a16f2f2f.jpg  \n",
       "b81a0a45f9b1ee97  ./data/landmarks/test/test/b81a0a45f9b1ee97.jpg  \n",
       "570e28cc63fab858  ./data/landmarks/test/test/570e28cc63fab858.jpg  \n",
       "b8bc63608b5fef1a  ./data/landmarks/test/test/b8bc63608b5fef1a.jpg  \n",
       "54cfd1f5f683b966  ./data/landmarks/test/test/54cfd1f5f683b966.jpg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_info_full = pd.read_csv('./data/test.csv', index_col='id')\n",
    "test_info_full.head()\n",
    "\n",
    "test_info = test_info_full.loc[test_image_ids]\n",
    "test_info['filename'] = pd.Series(test_images, index=test_image_ids)\n",
    "\n",
    "test_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cat = 203094 #number of unique classes (yikes)\n",
    "n_valid_classes = 23215\n",
    "input_shape = (99,99)\n",
    "batch_size = 48\n",
    "batch_size_predict = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "one_hot_encoder = OneHotEncoder(sparse=True, n_values=n_cat)\n",
    "\n",
    "train['label'] = label_encoder.fit_transform(train['landmark_id'].values)\n",
    "train['one_hot'] = one_hot_encoder.fit_transform(\n",
    "                    train['label'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(info, input_shape = input_shape):\n",
    "    input_shape = tuple(input_shape)\n",
    "    imgs = np.zeros((len(info), input_shape[0], input_shape[1], 3))\n",
    "\n",
    "    for i in range(len(info)):\n",
    "        fname = info.iloc[i]['filename']\n",
    "        try:\n",
    "            img = cv2.cvtColor(\n",
    "                  cv2.resize(cv2.imread(fname),input_shape),\n",
    "                  cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            warnings.warn('Warning: could not read image: '+ fname +\n",
    "                          '. Use black img instead.')\n",
    "            img = np.zeros((input_shape[0], input_shape[1], 3))\n",
    "        imgs[i,:,:,:] = img\n",
    "    \n",
    "    return imgs\n",
    "def load_cropped_images(info, crop_p=0.2, crop='random'):\n",
    "    new_res = np.array([int(input_shape[0]*(1+crop_p)), int(input_shape[1]*(1+crop_p))])\n",
    "    if crop == 'random':\n",
    "        cx0 = np.random.randint(new_res[0] - input_shape[0], size=len(info))\n",
    "        cy0 = np.random.randint(new_res[1] - input_shape[1], size=len(info))\n",
    "    else:\n",
    "        if crop == 'central':\n",
    "            cx0, cy0 = (new_res - input_shape) // 2                \n",
    "        if crop == 'upper left':\n",
    "            cx0, cy0 = 0, 0\n",
    "        if crop == 'upper right':\n",
    "            cx0, cy0 = new_res[1] - input_shape[1], 0\n",
    "        if crop == 'lower left':\n",
    "            cx0, cy0 = 0, new_res[0] - input_shape[0]\n",
    "        if crop=='lower right':\n",
    "            cx0, cy0 = new_res - input_shape        \n",
    "        cx0 = np.repeat(np.expand_dims(cx0, 0), len(info))\n",
    "        cy0 = np.repeat(np.expand_dims(cy0, 0), len(info))\n",
    "\n",
    "    cx1 = cx0 + input_shape[0]\n",
    "    cy1 = cy0 + input_shape[1]\n",
    "    \n",
    "    raw_imgs = load_images(info, input_shape=tuple(new_res))\n",
    "    \n",
    "    cropped_imgs = np.zeros((len(info), input_shape[0], input_shape[1], 3))\n",
    "    for ind in range(len(info)):\n",
    "        cropped_imgs[ind,:,:,:] = raw_imgs[ind,\n",
    "                                           cy0[ind]:cy1[ind],\n",
    "                                           cx0[ind]:cx1[ind], :]\n",
    "    \n",
    "    return cropped_imgs\n",
    "\n",
    "\n",
    "def get_image_gen(info_arg, \n",
    "                  shuffle=True, \n",
    "                  image_aug=True, \n",
    "                  eq_dist=False, \n",
    "                  n_ref_imgs=16, \n",
    "                  crop_prob=0.5, \n",
    "                  crop_p=0.5):\n",
    "    if image_aug:\n",
    "        datagen = ImageDataGenerator(\n",
    "            rotation_range=4.,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.5,\n",
    "            channel_shift_range=25,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest')\n",
    "        \n",
    "        if crop_prob > 0:\n",
    "            datagen_crop = ImageDataGenerator(\n",
    "                rotation_range=4.,\n",
    "                shear_range=0.2,\n",
    "                zoom_range=0.1,\n",
    "                channel_shift_range=20,\n",
    "                horizontal_flip=True,\n",
    "                fill_mode='nearest')\n",
    "        \n",
    "    count = len(info_arg)\n",
    "    while True:\n",
    "        if eq_dist:\n",
    "            def sample(df):\n",
    "                return df.sample(min(n_ref_imgs, len(df)))\n",
    "            info = info_arg.groupby('landmark_id', group_keys=False).apply(sample)\n",
    "        else:\n",
    "            info = info_arg\n",
    "        print('Generate', len(info), 'for the next round.')\n",
    "        \n",
    "        #shuffle data\n",
    "        if shuffle and count >= len(info):\n",
    "            info = info.sample(frac=1)\n",
    "            count = 0\n",
    "            \n",
    "        # load images\n",
    "        for ind in range(0,len(info), batch_size):\n",
    "            count += batch_size\n",
    "\n",
    "            y = info['landmark_id'].values[ind:(ind+batch_size)]\n",
    "            \n",
    "            if np.random.rand() < crop_prob:\n",
    "                imgs = load_cropped_images(info.iloc[ind:(ind+batch_size)], \n",
    "                                           crop_p=crop_p*np.random.rand() + 0.01, \n",
    "                                           crop='random')\n",
    "                if image_aug:\n",
    "                    cflow = datagen_crop.flow(imgs, \n",
    "                                              y, \n",
    "                                              batch_size=imgs.shape[0], \n",
    "                                              shuffle=False)\n",
    "                    imgs, y = next(cflow)                    \n",
    "            else:\n",
    "                imgs = load_images(info.iloc[ind:(ind+batch_size)])\n",
    "                if image_aug:\n",
    "                    cflow = datagen.flow(imgs, \n",
    "                                       y, \n",
    "                                       batch_size=imgs.shape[0], \n",
    "                                       shuffle=False)\n",
    "                    imgs, y = next(cflow)             \n",
    "\n",
    "            imgs = preprocess_input(imgs)\n",
    "    \n",
    "            y_l = label_encoder.transform(y[y>=0.])        \n",
    "            y_oh = np.zeros((len(y), n_cat))\n",
    "            y_oh[y >= 0., :] = one_hot_encoder.transform(y_l.reshape(-1,1)).todense()\n",
    "                    \n",
    "            yield imgs, y_oh\n",
    "            \n",
    "train_gen = get_image_gen(train, \n",
    "                          eq_dist=True, \n",
    "                          n_ref_imgs=512, \n",
    "                          crop_prob=0.3, \n",
    "                          crop_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DISTANCE_THRESHOLD = 0.8\n",
    "\n",
    "def load_delf_features(img_id, ddir):\n",
    "    locations = np.load(ddir + img_id + '_loc.npy')\n",
    "    descriptions = np.load(ddir + img_id + '_desc.npy')\n",
    "    return locations, descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_imgs_1_preloaded(locations_1, \n",
    "                             descriptors_1, \n",
    "                             d1_tree, \n",
    "                             img_id_2, \n",
    "                             dir_2='delf-train/'):\n",
    "    # Read features.\n",
    "    num_features_1 = locations_1.shape[0]\n",
    "    locations_2, descriptors_2 = load_delf_features(img_id_2, dir_2)\n",
    "    num_features_2 = locations_2.shape[0]\n",
    "\n",
    "    if len(locations_1)*len(locations_2)==0:\n",
    "        return 0\n",
    "    \n",
    "    _, indices = d1_tree.query(\n",
    "            descriptors_2, \n",
    "            distance_upper_bound=_DISTANCE_THRESHOLD)\n",
    "\n",
    "    if len(indices)==0:\n",
    "        return 0\n",
    "    \n",
    "    # Select feature locations for putative matches.       \n",
    "    locations_2_to_use = np.array([\n",
    "      locations_2[i,]\n",
    "      for i in range(num_features_2)\n",
    "      if indices[i] != num_features_1\n",
    "    ])\n",
    "    locations_1_to_use = np.array([\n",
    "      locations_1[indices[i],]\n",
    "      for i in range(num_features_2)\n",
    "      if indices[i] != num_features_1\n",
    "    ])\n",
    "\n",
    "    # Perform geometric verification using RANSAC.                                                   \n",
    "    \n",
    "    if len(locations_1_to_use)*len(locations_2_to_use)==0:\n",
    "        return 0\n",
    "\n",
    "    _, inliers = ransac(\n",
    "      (locations_1_to_use, locations_2_to_use),\n",
    "      AffineTransform,\n",
    "      min_samples=3,\n",
    "      residual_threshold=20,\n",
    "      max_trials=1000)\n",
    "\n",
    "    if inliers is None:\n",
    "        score = 0.\n",
    "    else:\n",
    "        score = sum(inliers)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_imgs(img_id_1, \n",
    "                 img_id_2, \n",
    "                 dir_1, dir_2='delf-train/', \n",
    "                 plot=False, \n",
    "                 img_dir_1=None, \n",
    "                 img_dir_2=None):\n",
    "    # Read features.\n",
    "    _DISTANCE_THRESHOLD = 0.8\n",
    "    locations_1, descriptors_1 = load_delf_features(img_id_1, dir_1)\n",
    "    num_features_1 = locations_1.shape[0]\n",
    "    \n",
    "    locations_2, descriptors_2 = load_delf_features(img_id_2, dir_2)\n",
    "    num_features_2 = locations_2.shape[0]\n",
    "    \n",
    "    if len(locations_1)*len(locations_2)==0:\n",
    "        return 0\n",
    "    \n",
    "    d1_tree = cKDTree(descriptors_1)\n",
    "    _, indices = d1_tree.query(\n",
    "      descriptors_2, distance_upper_bound=_DISTANCE_THRESHOLD)\n",
    "\n",
    "    # Select feature locations for putative matches.       \n",
    "    locations_2_to_use = np.array([\n",
    "      locations_2[i,]\n",
    "      for i in range(num_features_2)\n",
    "      if indices[i] != num_features_1\n",
    "    ])\n",
    "    locations_1_to_use = np.array([\n",
    "      locations_1[indices[i],]\n",
    "      for i in range(num_features_2)\n",
    "      if indices[i] != num_features_1\n",
    "    ])\n",
    "\n",
    "    # Perform geometric verification using RANSAC.                                                   \n",
    "#     try:  \n",
    "    _, inliers = ransac(\n",
    "          (locations_1_to_use, locations_2_to_use),\n",
    "          AffineTransform,\n",
    "          min_samples=3,\n",
    "          residual_threshold=20,\n",
    "          max_trials=1000)\n",
    "\n",
    "    if len(locations_1_to_use)*len(locations_2_to_use)==0:\n",
    "        score = 0\n",
    "    elif inliers is None:\n",
    "        score = 0.   \n",
    "    else:\n",
    "        score = sum(inliers)       \n",
    "    \n",
    "#     if plot:\n",
    "#         _, ax = plt.subplots()\n",
    "#         img_1=cv2.cvtColor(\n",
    "#                     cv2.resize(cv2.imread(img_dir_1+img_id_1+'.jpg'),input_shape),\n",
    "#                     cv2.COLOR_BGR2RGB)/255.\n",
    "#         img_2=cv2.cvtColor(\n",
    "#                     cv2.resize(cv2.imread(img_dir_2+img_id_2+'.jpg'),input_shape),\n",
    "#                     cv2.COLOR_BGR2RGB)/255. \n",
    "        \n",
    "#         inlier_idxs = np.nonzero(inliers)[0]\n",
    "#         plot_matches(\n",
    "#           ax,\n",
    "#           img_1,\n",
    "#           img_2,\n",
    "#           locations_1_to_use,\n",
    "#           locations_2_to_use,\n",
    "#           np.column_stack((inlier_idxs, inlier_idxs)),\n",
    "#           matches_color='b')\n",
    "#         ax.axis('off')\n",
    "#         ax.set_title('DELF correspondences')\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_features(locations_1, descriptors_1, locations_2, descriptors_2, d1_tree):\n",
    "\n",
    "    num_features_1 = locations_1.shape[0]\n",
    "    num_features_2 = locations_2.shape[0]\n",
    "    \n",
    "    if num_features_1 * num_features_2 == 0:\n",
    "        return 0\n",
    "    \n",
    "    _, indices = d1_tree.query(\n",
    "      descriptors_2, distance_upper_bound=_DISTANCE_THRESHOLD)\n",
    "\n",
    "    if len(indices)==0:\n",
    "        return 0\n",
    "    \n",
    "    # Select feature locations for putative matches.           \n",
    "    locations_2_to_use = np.array([\n",
    "      locations_2[i,]\n",
    "      for i in range(num_features_2)\n",
    "      if indices[i] != num_features_1\n",
    "    ])\n",
    "    locations_1_to_use = np.array([\n",
    "      locations_1[indices[i],]\n",
    "      for i in range(num_features_2)\n",
    "      if indices[i] != num_features_1\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Perform geometric verification using RANSAC.                                                   \n",
    "    if len(locations_1_to_use) * len(locations_2_to_use) == 0:\n",
    "        return 0\n",
    "    \n",
    "    _, inliers = ransac(\n",
    "      (locations_1_to_use, locations_2_to_use),\n",
    "      AffineTransform,\n",
    "      min_samples=3,\n",
    "      residual_threshold=20,\n",
    "      max_trials=1000)\n",
    "\n",
    "    if inliers is None:\n",
    "        score = 0.\n",
    "    else:\n",
    "        score = sum(inliers)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score=compare_imgs(train[train['landmark_id'] == 4831].sample(1).index[0],\n",
    "#                    train[train['landmark_id'] == 4302].sample(1).index[0], \n",
    "#                    dir_1='delf-train/', \n",
    "#                    dir_2='delf-train/', \n",
    "#                    img_dir_1='./data/landmarks/train/train/', \n",
    "#                    img_dir_2='./data/landmarks/train/train/')\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_train_path = './delf-train/'\n",
    "r_train_image_files = glob.glob(r_train_path + '*_loc.npy')\n",
    "r_train_image_ids = [image_file.replace(\n",
    "    '_loc.npy', '').replace(r_train_path, '') for image_file in r_train_image_files]\n",
    "r_train_info=train.loc[r_train_image_ids]\n",
    "r_train_info['filename'] = pd.Series(dev_image_files, index=dev_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>landmark_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "      <th>one_hot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9e10f9e254ae67c8</th>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>10562</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10562</td>\n",
       "      <td>(0, 142820)\\t1.0\\n  (1, 104169)\\t1.0\\n  (2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fa2eaf85a62f7271</th>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>8262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8262</td>\n",
       "      <td>(0, 142820)\\t1.0\\n  (1, 104169)\\t1.0\\n  (2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8933414bc50c4d25</th>\n",
       "      <td>http://upload.wikimedia.org/wikipedia/commons/...</td>\n",
       "      <td>3591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3591</td>\n",
       "      <td>(0, 142820)\\t1.0\\n  (1, 104169)\\t1.0\\n  (2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81d9b65c9b38d5c1</th>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>16912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16912</td>\n",
       "      <td>(0, 142820)\\t1.0\\n  (1, 104169)\\t1.0\\n  (2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48e2cd96847945e7</th>\n",
       "      <td>https://upload.wikimedia.org/wikipedia/commons...</td>\n",
       "      <td>7201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7201</td>\n",
       "      <td>(0, 142820)\\t1.0\\n  (1, 104169)\\t1.0\\n  (2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                url  \\\n",
       "id                                                                    \n",
       "9e10f9e254ae67c8  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "fa2eaf85a62f7271  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "8933414bc50c4d25  http://upload.wikimedia.org/wikipedia/commons/...   \n",
       "81d9b65c9b38d5c1  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "48e2cd96847945e7  https://upload.wikimedia.org/wikipedia/commons...   \n",
       "\n",
       "                  landmark_id  filename  label  \\\n",
       "id                                               \n",
       "9e10f9e254ae67c8        10562       NaN  10562   \n",
       "fa2eaf85a62f7271         8262       NaN   8262   \n",
       "8933414bc50c4d25         3591       NaN   3591   \n",
       "81d9b65c9b38d5c1        16912       NaN  16912   \n",
       "48e2cd96847945e7         7201       NaN   7201   \n",
       "\n",
       "                                                            one_hot  \n",
       "id                                                                   \n",
       "9e10f9e254ae67c8    (0, 142820)\\t1.0\\n  (1, 104169)\\t1.0\\n  (2, ...  \n",
       "fa2eaf85a62f7271    (0, 142820)\\t1.0\\n  (1, 104169)\\t1.0\\n  (2, ...  \n",
       "8933414bc50c4d25    (0, 142820)\\t1.0\\n  (1, 104169)\\t1.0\\n  (2, ...  \n",
       "81d9b65c9b38d5c1    (0, 142820)\\t1.0\\n  (1, 104169)\\t1.0\\n  (2, ...  \n",
       "48e2cd96847945e7    (0, 142820)\\t1.0\\n  (1, 104169)\\t1.0\\n  (2, ...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_train_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1 = load_delf_features(r_train_info.index[1], 'delf-train/')\n",
    "x2,y2 = load_delf_features(r_train_info.index[2], 'delf-train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
